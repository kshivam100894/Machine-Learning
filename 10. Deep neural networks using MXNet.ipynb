{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks using MXNet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In multiclass logistic regression we mapped our inputs to the outputs using a single linear transformation.\n",
    "\n",
    "If the labels are related to the input data by an approximately function, then this approach might be adequate. But <I>linearity is a strong assumption</I>. Linearity means that given an output of interest, for each input, increasing the value of the input should either drive the value of the output up or drive it down, irrespective of the value of the other inputs.\n",
    "\n",
    "Imagine the case of classifying cats and dogs based on black and white images. That's like saying that for each pixel, increasing its value either increases the probability that it depicts a dog or decreases it. That's not reasonable. After all, there are both black dogs and black cats, and both white dogs and white cats.\n",
    "\n",
    "In order to figure out what is depicted in the image generally requires allowing more complex relationships between our inputs and outputs, considering the possibility that our pattern might be characterized by interactions among the many features. In such cases the linear model, will have <I>low accuracy</I>. We can model a more general class of functions by incorporating one or more <I>hidden layers</I>. The most easiest way to do this is to stack a bunch of layers of neurons on top of each other. Each layer feeds into the layer above it unless we generate an output. This architecture is called \"multilayer perceptron\". With an MLP, we are going to stack a bunch of layers on top of each other.\n",
    "$$\\begin{eqnarray}\n",
    "         h_1 &=& \\phi(W_1(x) + b_1)\\\\\n",
    "         h_2 &=& \\phi(W_2(h_1) + b_2)\\\\\n",
    "         \\dots \\\\ \n",
    "         h_n &=& \\phi(W_nh_{n-1} + b_n)\n",
    "  \\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Note that each layer requires its own set of parameters. For each hidden layer, we calculate its value by first applying a linear function to the activations of the layers below, and then applying an element-wise nonlinear activation function. Here, I have denoted the activation functions for the hidden layers as $\\phi$. Finally, given the topmost hidden layer, we will generate an output. Because, we are still focussing on multiclass classification, we will stick with the softmax activation in the output layer.\n",
    "$$\n",
    "     \\bar y = softmax(W_yh_n + b_y)\n",
    "$$\n",
    "\n",
    "Multilayer perceptrons can account for complex interactions in the inputs because the hidden neurons depend on the values of each of the inputs. It's even widely known that multilayer perceptrons are universal approximators. That means that even for a single-hidden-layer neural network, with enough nodes, and the right set of weights, it could model any function at all! Actually learning those functions is the hard part. And it turns out that we can approximate functions much more compactly if we use deeper (vs wider) neural networks. In this example, we will implement a multilayer perceptron with two hidden layers and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the relevant libraries\n",
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "data_ctx = ctx\n",
    "model_ctx = ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "batch_size = 64\n",
    "num_examples = 60000\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "train_data = gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train = 1, transform = transform), batch_size, shuffle = 1)\n",
    "test_data = gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train = 0, transform = transform), batch_size, shuffle = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some constants so it's easy to modify the network later\n",
    "num_hidden = 256\n",
    "weight_scale = .01\n",
    "\n",
    "#Allocate the parameters for the first hidden layer\n",
    "W1 = nd.random_normal(shape = (num_inputs, num_hidden), scale = weight_scale, ctx = model_ctx)\n",
    "b1 = nd.random_normal(shape = num_hidden, scale = weight_scale, ctx = model_ctx)\n",
    "\n",
    "# Allocate parameters for the second hidden layer\n",
    "W2 = nd.random_normal(shape = (num_hidden, num_hidden), scale = weight_scale, ctx = model_ctx)\n",
    "b2 = nd.random_normal(shape = num_hidden, scale = weight_scale, ctx = model_ctx)\n",
    "\n",
    "# Allocate parameters for the output layer\n",
    "W3 = nd.random_normal(shape = (num_hidden, num_outputs), scale = weight_scale, ctx = model_ctx)\n",
    "b3 = nd.random_normal(shape = num_outputs, scale = weight_scale, ctx = model_ctx)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocating space for each parameter's gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions:\n",
    "\n",
    "If we compose multi-layer network but use only linear operations then our entire network will still be a linear function.\n",
    "That's because $\\bar y = X \\cdot W_1 \\cdot W_2 \\cdot W_3 = X \\cdot W_4$ for $ W_4 = W_1 \\cdot W_2 \\cdot W_3$.\n",
    "To give our model the capacity to capture non-linear functions, we will need to interleave our linear operations with\n",
    "activation functions.<br>\n",
    "In this case we'll use <b>the rectified linear unit (ReLU)</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, nd.zeros_like(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Output:\n",
    "\n",
    "As with multiclass logistic regression, we will want the output to be a valid probability distribution.\n",
    "We'll use the <b>softmax activation</b> function on our output to make sure that our outputs sum to 1 and are non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear - nd.max(y_linear))\n",
    "    partition = nd.nansum(exp, axis = 0, exclude = 1).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the output through cross-entropy loss function\n",
    "def cross_entropy(yhat, y):\n",
    "    return -nd.nansum(y*nd.log(yhat), axis = 0, exclude = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax cross-entropy loss function:\n",
    "\n",
    "The softmax function calculates $\\bar y_j = \\frac {e^{z_j}}{\\sum_{i=1}^ne^{z_i}}$, where $\\bar y_j$ is the j-th element of the input `yhat` variable in `cross-entropy` function and $z_j$ is the j-th element of the input `y_linear` variable in function `softmax`.\n",
    "\n",
    "If some $z_i$ is very large (i.e. very positive), then $e^{z_i}$ might be larger than the largest number we can have for certain types of `float` (i.e. overflow). This would make the denominator (and/or numerator) `infinite` and we get zero, or `infinite` or `nan` for $\\bar y_j$. In any case we won't get a well defined return value for `cross_entropy`. This is the reason we subtract $max(z_i)$ from all $z_i$ first in `softmax` function. We can verify that this shifting in $z_i$ will not change the return value of `softmax`.\n",
    "\n",
    "After the above subtraction/multiplication step, it is possible that $z_j$ is very negative. Thus, $e^{z_j}$ will be close to zero and might be rounded to zero due to finite precision (i.e. underflow), which makes $\\bar y_j$ zero and we get `-infinite` for $log(\\bar y_j)$. A few steps down the road in backpropagation it starts to get horrific not-a-number (`nan`) results printed to screen. \n",
    "\n",
    "Our salvation is that even though we are computing these exponential functions, we ultimately plan to take their log in the cross_entropy functions. It turns out that by combining the `softmax` and `cross_entropy` equations we can elude the numerical stability issues that otherwise might plague us during backpropagation. As shown in the equation below, we avoided calculating $e^{z_j}$ but directly used $z_j$ due to $log(exp(\\cdot))$.\n",
    "$$\n",
    "    log(\\bar y_j) = log \\left(\n",
    "                               \\frac {e^{z_j}} {\\sum_{i=1}^n e^{z_i}}\n",
    "                         \\right)\n",
    "                  = log(e^{z_j}) - log \\left(\\sum_{i=1}^n e^{z_i}\\right)\n",
    "                  = z_j - log \\left(\\sum_{i=1}^n e^{z_i}\\right)\n",
    "$$\n",
    "              \n",
    "Now instead of passing softmax probabilities into our new loss function, we will just pass our `yhat_linear` and compute the softmax and its log all at once inside the softmax_cross_entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return -nd.nansum(y * nd.log_softmax(yhat_linear), axis = 0, exclude = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    # Compute the first hidden layer\n",
    "    h1_linear = nd.dot(X, W1) + b1\n",
    "    h1 = relu(h1_linear)\n",
    "    \n",
    "    # Compute the second hidden layer\n",
    "    h2_linear = nd.dot(h1, W2) + b2\n",
    "    h2 = relu(h2_linear)\n",
    "    \n",
    "    # Compute the output layer.\n",
    "    # We will omit the softmax function here\n",
    "    # because it will be applied in the softmax_cross_entropy loss\n",
    "    yhat_linear = nd.dot(h2, W3) + b3\n",
    "    return yhat_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr*param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis = 1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 1.2316596712430319, Train_acc: 0.87943333, Test_acc: 0.8821\n",
      "Epoch: 1. Loss: 0.33631670532226565, Train_acc: 0.9204, Test_acc: 0.9201\n",
      "Epoch: 2. Loss: 0.22615769240061442, Train_acc: 0.9460667, Test_acc: 0.9448\n",
      "Epoch: 3. Loss: 0.16544468241532645, Train_acc: 0.9619833, Test_acc: 0.9581\n",
      "Epoch: 4. Loss: 0.12941680086652438, Train_acc: 0.9691833, Test_acc: 0.965\n",
      "Epoch: 5. Loss: 0.10541769832670689, Train_acc: 0.9738333, Test_acc: 0.967\n",
      "Epoch: 6. Loss: 0.0885169897776097, Train_acc: 0.9789, Test_acc: 0.9705\n",
      "Epoch: 7. Loss: 0.07580567805568378, Train_acc: 0.983, Test_acc: 0.9729\n",
      "Epoch: 8. Loss: 0.06475055932551622, Train_acc: 0.98536664, Test_acc: 0.9739\n",
      "Epoch: 9. Loss: 0.057021041063219306, Train_acc: 0.9866167, Test_acc: 0.9736\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = .001\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch: %s. Loss: %s, Train_acc: %s, Test_acc: %s\" %\n",
    "         (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predictions are: \n",
      "[3. 8. 3. 5. 3. 5. 8. 5. 2. 6.]\n",
      "<NDArray 10 @cpu(0)>\n",
      "true labels: \n",
      "[3. 8. 3. 5. 3. 5. 2. 5. 2. 6.]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Defining the function to do prediction\n",
    "def model_predict(net, data):\n",
    "    output = net(data)\n",
    "    return nd.argmax(output, axis = 1)\n",
    "\n",
    "samples = 10\n",
    "\n",
    "mnist_test = mx.gluon.data.vision.MNIST(train=0, transform=transform)\n",
    "\n",
    "#let's sample 10 random data points from the test set\n",
    "sample_data = mx.gluon.data.DataLoader(mnist_test, samples, shuffle=1)\n",
    "for i, (data, label) in enumerate(sample_data):\n",
    "    data = data.as_in_context(model_ctx)\n",
    "    im = nd.transpose(data, (1,0,2,3))\n",
    "    im = nd.reshape(im, (28, 10*28, 1))\n",
    "    imtiles = nd.tile(im, (1,1,3))\n",
    "    \n",
    "    plt.imshow(imtiles.asnumpy())\n",
    "    plt.show()\n",
    "    pred = model_predict(net, data.reshape((-1, 784)))\n",
    "    print('model predictions are:', pred)\n",
    "    print('true labels:', label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "With just two hidden layers containing 256 hidden nodes, respectively, we can achieve over 95% accuracy on this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
