{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation with `autograd`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we <I>train</I> models to get better and as a function of experience. As discussed in the <I>gradient descent</I> notebook, <I>getting better</I> means minimizing a <I>loss function</I>, i.e., a score that answers \"how bad is our model?\" With machine learning models, we choose loss functions to be differentiable with respect to our parameters. Put simply, this means that for each of the model's parameters, we can determine how much <I>increasing</I> or <I>decreasing</I> it might affect the loss.\n",
    "\n",
    "<I>MXNet's</I> autograd expedites this work by automatically calculating derivatives. `mxnet.autograd` allows us to take derivatives while writing ordinary imperative code. Let's go through it step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching gradients:\n",
    "\n",
    "As a toy example, let's say we are interested in differentiating a function `f = 2 * (x**2)` with respect to parameter x. We can start by assigning an initial value of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nd.array([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we compute the gradient of `f` with respect to `x`, we'll need a place to store it. In MXNet, we can tell an NDArray that we plan to store a gradient by invoking its `attach_grad()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instruct MXNet to start recording by placing code inside a `with autograd.record():` block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    y = x*2\n",
    "    z = y*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's backprop by calling `z.backwar()`. When "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if this is the expected output. Remember that `y = x * 2`, and `z = x * y`, so `z` should be equal to `2 * x * x`. After, doing backprop with `z.backward()`, we expect to get back gradient dz/dx as follows: dy/dx = 2, dz/dx = 4\\*x. So, if everything went according to plan `x.grad` should consist of NDArray with the values `[[4, 8],[12, 16]]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 4.  8.]\n",
      " [12. 16.]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[10.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "mx.nd.sum(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
