{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Matrix based approach to compute the output of the Neural Network:\n",
    "\n",
    "We'll use the notation $w^l_{jk}$ to denote the connection from the $k^{th}$ neuron in $(l-1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer.\n",
    "We'll use $a^l_j$ to denote the activation of the $j^{th}$ neuron in the $l^{th}$ layer and $b^l_j$ to denote the bias of the $j^{th}$ neuron in the $l^{th}$ layer.\n",
    "With the above mentioned notations the activation:\n",
    "$$\n",
    "            a^l_j = \\sigma (\\sum_k w^l_{jk}a^{l-1}_{k} + b^l_j) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\dots (eqn\\;1)\n",
    "$$\n",
    "\n",
    "To rewrite this equation in a matrix form we define a weight matrix for each layer $l$ as $w^l$. The entries of the weight matrix $w^l$ are just the weights connecting to the $l^{th}$ layer, i.e., the entry in the $j^{th}$ row and $k^{th}$ column would be $w^l_{jk}$.\n",
    "\n",
    "Similarly, for each layer $l$ we define a bias vector $b^l$. The components of this vector are just the bias values $b^l_j$, one component for each neuron in the $l^{th}$ layer. \n",
    "Finally, we define an activation vector $a^l$ whose components are the activations $a^l_j$.\n",
    "We vectorize a function by applying the function to each element of the array. For example, for $f(x) = x^2$:\n",
    "$$\n",
    "f \\left( \\left[  \\begin{matrix} 2\\\\3 \\end{matrix} \\right] \\right) = \\left[ \\begin{matrix} f(2)\\\\ f(3) \\end{matrix} \\right] = \\left[ \\begin{matrix} 4\\\\\;\;9 \\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "With these notations $eqn\\;1$ can be written as:\n",
    "$$\n",
    "            a^l = \\sigma(w^l a^{l-1} + b^l)  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\dots     (eqn\\;2)\n",
    "$$\n",
    "            \n",
    "This is the quirk in using the notation $w^l_{jk}$ as if we would have used $w^l_{kj}$ instead we would have to take the transpose of the weight matrix to multiply it with the activation vector. Thus, using $w^l_{jk}$ lets us write the equation without taking a transpose.\n",
    "\n",
    "Now while computing $a^l$ we compute the intermediate quantity:\n",
    "$$\n",
    "        z^l = w^la^l + b^l\n",
    "$$\n",
    "$z^l$ is called the weighted input to the neurons in layer $l$.\n",
    "Thus $eqn\\;2$ can be written as: $a^l = \\sigma(z^l)$.\n",
    "Also, $z^l$ has components: $z^l_j = \\sum_k w^l_{jk}a^l_k + b^l_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two assumptions we need about the cost function:\n",
    "\n",
    "The goal of backpropagation is to compute the partial derivatives ${\\partial C}$/${\\partial w}$ and ${\\partial C}$/${\\partial b}$ of the cost function $C$ with respect to any weight $w$ or bias $b$ in the network.\n",
    "\n",
    "To take an example cost function, we will consider the mean squared error function:\n",
    "$$\n",
    "    C = \\frac {1}{2n}\\sum_x\\|y(x) - a^L(x)\\|^2\n",
    "$$\n",
    "    \n",
    "where $n$ is the total number of examples; the sum is over individual training examples, $x$; $y = y(x)$ is the corresponding desired output; $L$ denotes the number of layers in the network; and $a^L = a^L(x)$ is the vector of output activations when $x$ is the input.\n",
    "\n",
    "##### Assumption 1: \n",
    "The cost function can be written as an average: $C = \\frac {1}{n}\\sum_xC_x$ for individual training examples x. This holds true for the quadractic cost function.\n",
    "              \n",
    "What backpropagation does is that it lets us compute the partial derivatives ${\\partial C_x}$/${\\partial w}$ and ${\\partial C_x}$/${\\partial b}$ for a single training example. We then calculate  ${\\partial C}$/${\\partial w}$ and ${\\partial C}$/${\\partial b}$  by averaging over individual training examples. With this assumption in mind we suppose that the training example $x$ has been fixed, and we can drop the $x$ subscript and write $C_x$ as $C$. We'll eventually put $x$ back in, but for now it is a notational nuisance that is better left implicit.\n",
    "\n",
    "##### Assumption 2: \n",
    "The second assumption we make about the cost is that it can be written as a function of outputs from the                   neural network:\n",
    "$$\n",
    "                      C = C(a^L)\n",
    "$$\n",
    "\n",
    "For example, the quadratic cost function satisfies this requirement, since the quadratic cost for a single training example may be written as:\n",
    "$$\n",
    "C = \\frac {1}{2}\\|y - a^L\\|^2 = \\frac{1}{2}\\sum_j(y_j-a^L_j)^2,\n",
    "$$\n",
    "and thus is a function of output activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadamard Product:\n",
    "Elementwise multiplication of two vectors of the same dimension is called Hadamard Product or Schur Product.\n",
    "\n",
    "I will be representing Hadamard product using '$\\odot$' notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four fundamental equations behind backpropagation:\n",
    "\n",
    "Backpropagation is about understanding how changing the weights and biases in the network affects the cost of the \n",
    "function. Ultimately, this is calculated by calculating $\\partial C$/$\\partial w^l_{jk}$ and $\\partial C$/$\\partial b^l_j$. To calculate the aforementioned derivatives, we will define an error term as $\\delta^l_j$ which we call the error in the $j^{th}$ neuron in $l^{th}$ layer.\n",
    "\n",
    "To understand the term $\\delta^l_j$, suppose if we add a little change to the neuron's weigted input term: $\\Delta z^l_j$, so that instead of outputting $\\sigma (z^l_j)$, the neuron instead outputs $\\sigma(z^l_j + \\Delta z^l_j)$. This change will now propagate through the entire network thus finally changing the cost by $\\frac {\\partial C}{\\partial z^l_j}\\Delta z^l_j$.\n",
    "\n",
    "Now we are trying to find a $\\Delta z^l_j$ which makes the cost smaller. If $\\frac {\\partial C}{\\partial z^l_j}$ has a large value (either positive or negative) then we can reduce the cost by quite a bit by choosing $\\Delta z^l_j$ to have a sign opposite to $\\frac {\\partial C}{\\partial z^l_j}$. By contrast, if $\\frac {\\partial C}{\\partial z^l_j}$ is already close to zero then we can't change the cost much at all by perturbing the weighted input $z^l_j$. In this case all we can say is that the neuron is already pretty near optimal. So there's a heuristic sense in which $\\frac {\\partial C}{\\partial z^l_j}$ is a measure of the error in the neuron.\n",
    "\n",
    "Hence, we define the error in $j^{th}$ neuron in $l^{th}$ layer as:\n",
    "$$\n",
    "            \\delta^l_j \\equiv \\frac{\\partial C}{\\partial z^l_j}\n",
    "$$\n",
    "As per our usual convention throughout the notebook we will use $\\delta^l$ to denote the vector of errors through layer $l$. <b>Backpropagation will give us a way to compute these errors for every layer $l$ and then relating these errors to the real quantities of interest $\\partial C$/$\\partial w^l_{jk}$ and $\\partial C$/$\\partial b^l_j$</b>.\n",
    "\n",
    "Backpropagation is based around four fundamental equations. Together these equations give us a way to compute the error term, $\\delta^l$ and the gradient of the cost function.\n",
    "\n",
    "1. <b>An eqn for error in the output layer, $\\delta^L$</b>:\n",
    "$$\n",
    "        \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j}\\sigma'(z^L_j)   \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\dots (eqn \\;BP1)\n",
    "$$\n",
    "\n",
    "    <b>Derivation</b>:\n",
    "    $$\n",
    "        \\delta^L_j = \\frac{\\partial C}{\\partial z^L_j}\n",
    "    $$\n",
    "    \n",
    "    Applying the chain rule, we can re-express the partial derivative above in terms of partial derivatives with respect to the output activations,\n",
    "    $$\n",
    "         \\delta^L_j = \\sum_k \\frac{\\partial C}{\\partial a^L_k} \\frac{\\partial a^L_k}{\\partial z^L_j},\n",
    "    $$\n",
    "    Now, the output activation $a^L_k$ of the $k^{th}$ neuron will depend on the weighted input $z^l_j$ for the $j^{th}$ neuron only when $k = j$, otherwise it will be zero. Hence, the above eqn for error can be further simplified and written           as:\n",
    "    $$\n",
    "        \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_k}{\\partial z^L_j},\n",
    "    $$\n",
    "        \n",
    "   Recalling that $a^L_j = \\sigma(z^L_j)$, the second term on the right can be written as $\\sigma'(z^L_j)$ and the equation becomes\n",
    "   $$\n",
    "        \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j}\\sigma'(z^L_j)\n",
    "   $$\n",
    "\n",
    "    In the above eqn, the first term is measuring that how fast the cost is changing as a function of the activation of the     $j^{th}$ neuron. If, for example, $C$ doesn't depend on a particular output neuron, $j$, then $\\delta^l_j$ will be small, which is       what we would expect. The second term on the right is measuring how fast the activation function is changing at  $z^L_j$.<br>\n",
    "    In this notebook, we are considering, $C = \\frac {1}{2} \\sum_j(y_j-a^L_j)^2$. <br>\n",
    "    Hence, $\\partial C/\\partial a^L_j = (a^L_j - y_j)$.\n",
    "    \n",
    "    Equation $BP1$ is a component-wise expression for $\\delta^L$. It's a perfectly good expression, but not the matrix based form we want for backpropagation.However, $eqn \\;BP1$ can be written in matrix form as:\n",
    "    $$\n",
    "    \\delta^L = \\nabla_aC \\odot \\sigma'(z^L)\n",
    "    $$\n",
    "    \n",
    "    Here, $\\nabla_aC$ is defined to be a vector whose components are the partial derivatives $\\partial C/\\partial a^L_j$.\n",
    "    Now, in case of quadratic cost , we have $\\nabla_aC = (a^L - y)$, and so fully matrix based form of the $eqn \\;BP1$ becomes:\n",
    "    $$\n",
    "     \\delta^L = (a^L - y) \\odot \\sigma'(z^L)\n",
    "    $$\n",
    "    \n",
    " \n",
    "2. <b>An eqn for the error $\\delta^l$ in terms of error in the next layer $\\delta^{l+1}$</b>:\n",
    "$$\n",
    "       \\delta^l = ((w^{l+1})^T\\delta^{l+1})\\odot \\sigma'{z^l}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\dots (eqn\\; BP2)\n",
    "$$\n",
    "        \n",
    "    $Eqn \\;BP2$ helps us move the error backward as it enables us to calculate the error in the $l^{th}$ layer provided we know the     error at $(l+1)^th$ layer. Combining $eqn \\;BP1$ and $BP2$ we can calculate the error $\\delta^l$ for any layer in the network.\n",
    "    \n",
    "    <b>Derivation</b>:\n",
    "    \n",
    "    To prove $eqn \\;BP2$ we need to rewrite $\\delta^l_j = \\partial C/\\partial z^l_j$ in terms of $\\delta^{l+1}_k = \\partial C/\\partial z^{l+1}_k$. This can be done using the chain rule:\n",
    "      $$\\begin{eqnarray}\n",
    "               \\delta^l_j & = & \\frac {\\partial C}{\\partial z^l_j}\\\\\n",
    "                        &=& \\sum_k \\frac {\\partial C} {\\partial z^{l+1}_k} \\frac {\\partial z^{l+1}_k}{\\partial z^l_j}\\\\\n",
    "                       &=& \\sum_k \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} \\delta^{l+1}_k\n",
    "         \\end{eqnarray}\n",
    "       $$\n",
    "        \n",
    "    Now, \n",
    "$$\\begin{eqnarray}\n",
    "        z^{l+1}_k & = & \\sum_j w^{l+1}_{kj} a^l_j + b^{l+1}_k\\\\\n",
    "                  & = & \\sum_j w^{l+1}_{kj}\\sigma(z^l_j) + b^{l+1}_k\n",
    "           \\end{eqnarray}\n",
    "$$\n",
    "             \n",
    "    Differentiating, we obtain:\n",
    "$$      \n",
    "          \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = w^{l+1}_{kj}\\sigma'(z^l_j)\n",
    "$$  \n",
    "\n",
    "    Substituting the value for $\\frac{\\partial z^{l+1}_k}{\\partial z^l_j}$, we get:\n",
    "$$\n",
    "            \\delta^l_j = \\sum_k w^{l+1}_{kj} \\delta^{l+1}_k \\sigma'(z^l_j)\n",
    "$$\n",
    "    This is $eqn\\;BP2$ in component form.\n",
    "            \n",
    "\n",
    "3. <b>An eqn for rate of change of the cost with respect to any bias in the network</b>:\n",
    "$$\n",
    "        \\delta^l_j = \\frac {\\partial C}{\\partial b^l_j}  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\dots (eqn\\; BP3)\n",
    "$$      \n",
    "   <b>Derivation</b>:\n",
    "$$\n",
    "        \\delta^l_j = \\frac {\\partial C}{\\partial b^l_j} \\frac {\\partial b^l_j} {\\partial z^l_j}\n",
    "$$      \n",
    "      Now, $z^l_j = \\sum_k w^l_{jk}a^{l-1}_k + b^l_j$<br>\n",
    "      Hence, $\\frac {\\partial z^l_j}{\\partial b^l_j} = 1$<br>\n",
    "        \n",
    "      Thus, $\\delta^l_j = \\frac {\\partial C}{\\partial b^l_j}$\n",
    "        \n",
    "\n",
    "4. <b>An equation for rate of change of the cost with respect to any weight in the network</b>:\n",
    "      $$   \n",
    "       \\frac {\\partial C} {\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\dots  (eqn \\;BP4)\n",
    "      $$    \n",
    "  <b>Derivation</b>:\n",
    "  \n",
    "   $$\\begin{eqnarray}\n",
    "    \\delta^l_j & = & \\frac {\\partial C}{\\partial w^l_{jk}} \\frac {\\partial w^l_{jk}} {\\partial z^l_{jk}} \\\\\n",
    "    \\implies \\frac {\\partial C}{\\partial w^l_{jk}} & = & \\frac {\\partial w^l_{jk}} {\\partial z^l_{jk}} \\delta^l_j\n",
    "   \\end{eqnarray}\n",
    "    $$\n",
    "     Now, \n",
    "      $$\\begin{eqnarray}\n",
    "      z^l_j &=& \\sum_k w^l_{jk} a^{l-1}_k + b^l_j \\\\\n",
    "         \\implies    \\frac {\\partial z^l_j} {\\partial w^l_{jk}} &=&  a^{l-1}_k\n",
    "       \\end{eqnarray}\n",
    "      $$\n",
    "     Substituting the term, we get:\n",
    "      $$\n",
    "         \\frac {\\partial C} {\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j\n",
    "      $$\n",
    "           \n",
    "    $Eqn \\;BP4$ can be written in a less index heavy way as:\n",
    "    $$\n",
    "       \\frac {\\partial C}{\\partial w} = a_{in}\\delta_{out}\n",
    "    $$\n",
    "       \n",
    "     A nice consequence of $eqn\\; BP4$ is that when the input activation is small, $a_{in}\\approx 0$, the gradient term ${\\partial C}/{\\partial w}$ will also get small. In this case, we say that the weight term is learning slowly, meaning that it's not changing much during gradient descent.In other words, weights output from low activation neurons learn slowly.\n",
    "\n",
    "     If we look at the graph of the sigmoid function then we recognize that the sigmoid function becomes very flat when its value is 1 or 0. When this occurs $\\sigma'(z^l_j \\approx 0$. And so the lesson is that a weight in the final layer will learn slowly if the output neuron is either low activation or high activation.\n",
    "\n",
    "     Summing up, we have learnt that the weight will learn slowly if either the input neuron is low activation, or if the output neuron has saturated,i.e., is either low or high activation.\n",
    "\n",
    "We can further turn this type of reasoning around. The four fundamental equations turn out to hold for any activation function, not just the standard sigmoid function, as is evident in the proofs. And so we can use these functions to design activation functions which have particular desired learning properties. As an example, if we were to choose a non-sigmoid activation function so that its derivative is always positive, and never gets close to 0. That would prevent the slow down of learning that happens when ordinary sigmoid neurons saturate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The backpropagation algorithm:\n",
    "\n",
    "The backpropagation equations provide us with a way of computing the gradient of the cost function. Let's explicitly write this out in the form of an algorithm:\n",
    "\n",
    "1. <b>Input</b><br>\n",
    "   $x$: Set the corresponding activation $a^1$ for the input layer.\n",
    "   \n",
    "2. <b>Feedforward:</b> For each $l = 2,3,\\dots,L$ compute $z^l = w^la^{l-1}+b^l$ and $a^l=\\sigma(z^l)$.\n",
    "\n",
    "3. <b>Output error</b><br>\n",
    "   $\\delta^L$: Compute the vector $\\delta^L = \\nabla_aC \\odot \\sigma'(z^L)$.\n",
    " \n",
    "4. <b>Backpropagate the error:</b> For each $L = L-1, L-2,\\dots,2$ compute $\\delta^l = ((w^{l+1})^T\\delta{l+1})\\odot \\sigma'(z^l)$.\n",
    "\n",
    "5. <b>Output:</b> The gradient of the cost function is given by $eqn\\;BP3$ and $eqn\\;BP4$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
