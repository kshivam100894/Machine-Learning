{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, we introduced logistic regression, a classic algorithm for performing binary classification. We implemented the model:\n",
    "$$\n",
    "    \\bar y = \\sigma (xw^T + b), where\\; \\sigma \\; is\\;the\\;sigmoid\\;squashing\\;function\n",
    "$$\n",
    "\n",
    "This activation function on the final layer was crucial because it forced our outputs to take values in the range $[0, 1]$. That allowed us to interpret these outputs as probabilities. We then updated our parameters to give the true labels (which take values either 1 or 0) the highest probability.\n",
    "\n",
    "Binary classification is quite useful. We can use it to predict spam vs. not spam or cancer vs not cancer. But not every problem fits the mold of binary classification. Sometimes we encounter a problem where each example could belong to one of $k$ classes. \n",
    "Given $k$ classes, the most naive way to solve a multiclass classification problem is to train $k$ different binary classifiers $f_i(x)$.\n",
    "We could then predict that an example $x$ belongs to the class $i$ for which the probability that the label applies is the highest: \n",
    "$$\n",
    "\\underset{i}{max}\\;f_i(x)\n",
    "$$\n",
    "\n",
    "The smarter way to go about this is to force the output layer to be a discrete probability distribution over $k$ classes.\n",
    "To be a valid probability distribution we would want the output $\\bar y$ to:\n",
    "1. Contain only non- negative values.\n",
    "2. Sum to 1.\n",
    "    \n",
    "We accomplish this by using the <I><b>softmax</b></I> function. Given an input vector $z$, softmax does two things. First, it exponentiates (elementwise) $e^z$, forcing all the values to be strictly positive. Then it normalizes so that all values sum to 1. Following the softmax operation computes the following:\n",
    "$$\n",
    "    softmax(z) = \\frac{e^z}{\\sum_{i=1}^k e^{z_i}}\n",
    "$$\n",
    "\n",
    "We'll need weights to connect each of our inputs to each of our k outputs.\n",
    "The weights can be represented one for each input and output node pair in a matrix $W$.\n",
    "We generate the linear mapping from inputs to outputs via matrix-vector product $xW + b$.\n",
    "Note that bias term ($b$) is now a vector, with one component for each output node.\n",
    "The multiclass classification model can be written as:\n",
    "$$\n",
    "    \\bar y = softmax(xW + b)\n",
    "$$\n",
    "\n",
    "This model is sometimes called the <I> multiclass logistic regression</I>. Other names for it include <I>softmax regression</I> and <I> multinomila regression</I>. For these concepts to sink in lets actually implement softmax regression. We will be using the multiclass classification model to classify hand-written digits this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "In mathematical notation we represent vectors with lower case alphabets and matrices with upper case alphabets.\n",
    "\n",
    "### Batch Training:\n",
    "\n",
    "Let's assume we have $d$ inputs and $k$ outputs. So the shapes of various variables explicitly would be:\n",
    "$$\n",
    "    \\underset {1\\times k}{z} = \\underset {1\\times d}{x} \\cdot \\underset{d \\times k} {W} + \\underset{1 \\times k} {b}\n",
    "$$\n",
    "    \n",
    "We will be one hot encoding our output labels, for example $\\bar y = 5$ would be $\\bar y_{one-hot} = [0,0,0,0,1,0,0,0,0,0]$ when one-hot encoded for a 10-class classification problem.\n",
    "    \n",
    "So $\\bar y = softmax(z)$ becomes:\n",
    "$$\n",
    "   \\underset {1\\times k}{\\bar y_{one-hot}} = softmax_{one-hot}(\\underset {1\\times k} {z})\n",
    "$$\n",
    "    \n",
    "Suppose if we input a batch of m training examples, we would have a matrix $\\underset {m \\times d}{X}$ that is vertical stacking of individual training examples $x_i$.\n",
    "\n",
    "$$\n",
    "    X = \\left[ \\begin{matrix}\n",
    "                x_1\\\\\n",
    "                x_2\\\\\n",
    "                \\vdots \\\\\n",
    "                x_m\n",
    "               \\end{matrix}\n",
    "         \\right]\n",
    "      = \\left[ \\begin{matrix}\n",
    "                  x_{11} & x_{12} & \\dots & x_{1d}\\\\\n",
    "                  x_{21} & x_{22} & \\dots & x_{2d}\\\\\n",
    "                  \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "                  x_{m1} & x_{m2} & \\dots & x_{md}\n",
    "                \\end{matrix}\n",
    "        \\right]\n",
    "$$\n",
    "\n",
    "Under this batch training situation, $\\bar y_{one-hot} = softmax(z)$ turns into:\n",
    "$$\n",
    "    Y = softmax(Z) = softmax(XW + B)\n",
    "$$\n",
    "    \n",
    "where matrix $\\underset {m \\times k}{B}$ is formed by having $m$ copies of $b$ as follows:\n",
    "$$\n",
    "    B = \\left[ \\begin{matrix}\n",
    "                    b\\\\\n",
    "                    b\\\\\n",
    "                    \\vdots \\\\\n",
    "                    b\n",
    "                \\end{matrix}\n",
    "         \\right]\n",
    "      = \\left[ \\begin{matrix}\n",
    "                  b_1 & b_2 & \\dots & b_k\\\\\n",
    "                  b_1 & b_2 & \\dots & b_k\\\\\n",
    "                  \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "                  b_1 & b_2 & \\dots & b_k\n",
    "                \\end{matrix}\n",
    "         \\right]\n",
    "$$\n",
    "\n",
    "Each row of matrix $ \\underset {m \\times k}{Z}$ corresponds to one training example. The softmax function operates on each row of matrix $Z$ and returns a matrix $\\underset {m \\times k}{Y}$, each row of which corresponds to the one-hot encoded prediction of one training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd, gluon\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the compute context using MXNet\n",
    "#Feel free to change model_ctx to mx.gpu() if you have a well endowed machine\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()\n",
    "#model_ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with 28*28 centrally cropped black and white photograph of a hand-written digit.\n",
    "We'll use MXNet's utility to grab a copy of the dataset.\n",
    "The datasets accept a transform callback that can preprocess each item.\n",
    "Cast data and label to floats and normalize data to range $[0,1]$.\n",
    "We'll be using the test data for determining the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "mnist_train = gluon.data.vision.MNIST(train=1, transform = transform)\n",
    "mnist_test = gluon.data.vision.MNIST(train=0, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 parts of dataset for train and test. Each part has N items and each item is a tuple of an image and label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1) 5.0\n"
     ]
    }
   ],
   "source": [
    "image, label = mnist_train[0]\n",
    "print(image.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each image is formatted as a 3-tuple: (height, width, channel). For colored images, the channel would have 3 parameters: (Red, Green and Blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist_train))\n",
    "print(28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_examples = 60000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning libraries generally expect to get images in (batch, channel, height, width) format. However, most libraries for visualization prefer (height, width, channel). matplolib expects (height, width) or (height, width, channel) with RGB channels, so let's broadcast our single channel to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "im = mx.nd.tile(image, (1, 1, 3))\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(im.asnumpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading images into a data iterator\n",
    "batch_size = 64\n",
    "train_data = mx.gluon.data.DataLoader(mnist_train, batch_size, shuffle = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading iterator for the test_data\n",
    "test_data = mx.gluon.data.DataLoader(mnist_test, batch_size, shuffle = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will flatten each image into a single 1D vector with 28\\*28 = 784 components\n",
    "because our task is multiclass classification we will want to assign a probability to each of the classes $P(y = C|X)$ given the input $X$.\n",
    "In order to do this we are going to need one vector of 784 weights for each class, connecting each feature to the corresponding output.\n",
    "We'll also allocate one offset, called bias, for each of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nd.random_normal(shape = (num_inputs, num_outputs), ctx = model_ctx)\n",
    "b = nd.random_normal(shape = num_outputs, ctx = model_ctx)\n",
    "\n",
    "params = [W,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to let MXNet know we'll be expecting gradients corresponding to each of these parameters during training\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to linearly map our input $X$ onto 10 different real valued outputs `y_linear`.\n",
    "Then, before outputting these values, we'll want to normalize them so that they are non-negative and sum to 1.\n",
    "This normalization allows us to interpret the output $\\bar y$ as valid probability distibution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear - nd.max(y_linear, axis = 1).reshape((-1,1)))\n",
    "    norms = nd.sum(exp, axis = 1).reshape((-1,1))\n",
    "    return exp/norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.38854712 0.15868284 0.09794893 0.01202093 0.01595045 0.04553349\n",
      "  0.14224595 0.0314311  0.04231628 0.06532289]\n",
      " [0.08514825 0.43826103 0.02771222 0.08836042 0.17408127 0.05132056\n",
      "  0.02093019 0.06935012 0.03025253 0.01458343]]\n",
      "<NDArray 2x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "sample_y_linear = nd.random_normal(shape = (2,10))\n",
    "sample_yhat = softmax(sample_y_linear)\n",
    "print(sample_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1. 1.]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#confirming that the rows sum to 1\n",
    "print(nd.sum(sample_yhat, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X,W) + b\n",
    "    yhat = softmax(y_linear)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cross-entropy loss Funtion:\n",
    "\n",
    "The cost function being used here is called Cross Entropy:\n",
    "The basic idea is that we are going to take a target $Y$ that has been formatted as a one-hot vector, meaning one value\n",
    "corresponding to the correct label is set to 1 and others are set to 0, \n",
    "eg: $[0,1,0,0,0,0,0,0,0,0]$\n",
    "\n",
    "The basic idea of cross entropy loss is that we only care about how much probability is assigned to the correct label.\n",
    "In other words, we only care about the component of yhat assigned to label 2.\n",
    "Cross-entropy attempts to maximize the log-likelihood given to the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return -nd.sum(y * nd.log(yhat + 1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll be using the stochastic gradient descent optimizer\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr*param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation loop to calculate accuracy:\n",
    "\n",
    "Generally we define accuracy as the number of correct answers divided by the total number of questions.\n",
    "Lets write an evaluation loop that will take a data iterator and a network, returning a model's accuracy averaged over the\n",
    "entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis = 1)\n",
    "        numerator += nd.sum( predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1167"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Because we initialized our model randomly and roughly 1/10th of all examples belong to each of the 10 classes we should\n",
    "#have an accuracy close to 0.10\n",
    "evaluate_accuracy(test_data, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss 1.377373925558726, Train_acc 0.85815, Test_acc 0.8643\n",
      "Epoch 1. Loss 0.6138278275171916, Train_acc 0.88275, Test_acc 0.8819\n",
      "Epoch 2. Loss 0.518171291565895, Train_acc 0.89005, Test_acc 0.8865\n",
      "Epoch 3. Loss 0.46832715154886245, Train_acc 0.8976, Test_acc 0.8931\n",
      "Epoch 4. Loss 0.43529989680051806, Train_acc 0.9023833, Test_acc 0.9\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "learning_rate = 0.005\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape(-1,784)\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "        \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss %s, Train_acc %s, Test_acc %s\" % (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 28, 28, 1)\n",
      "(10, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predictions are: \n",
      "[2. 0. 1. 7. 6. 1. 6. 8. 6. 1.]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#We'll pick 10 random data points from the test dataset and use the trained model for predictions\n",
    "#Define the function to do prediction\n",
    "def model_predict(net, data):\n",
    "    output = net(data)\n",
    "    return nd.argmax(output, axis = 1)\n",
    "\n",
    "#sampling 10 data points from the test set\n",
    "sample_data = mx.gluon.data.DataLoader(mnist_test, 10, shuffle = 1)\n",
    "\n",
    "for i, (data, label) in enumerate(sample_data):\n",
    "    data = data.as_in_context(model_ctx)\n",
    "    print(data.shape)\n",
    "    im = nd.transpose(data, (1,0,2,3))\n",
    "    print(data.shape)\n",
    "    im = nd.reshape(im, (28, 10*28, 1))\n",
    "    imtiles = nd.tile(im, (1,1,3))\n",
    "    \n",
    "    plt.imshow(imtiles.asnumpy())\n",
    "    plt.show()\n",
    "    pred = model_predict(net, data.reshape((-1, 784)))\n",
    "    print('model predictions are:', pred)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "We can get 90% accuracy at this task just by training a linear model for a few seconds!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
