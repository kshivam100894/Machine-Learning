{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with gradient descent:\n",
    "\n",
    "Now that we have an insight into the functioning of the neuron, what we'd like is an algorithm which let's us find weights and biases so that the output from the network approximates the actual value, $y(x)$ for all training inputs $x$. To define how well we are achieving this goal, we define a cost function:\n",
    "$$\n",
    "        C(w, b) \\equiv \\frac{1}{2n}\\sum_x\\|y(x) - a\\|^2\n",
    "$$\n",
    "\n",
    "Here, $w$ denotes the collection of all weights in the network, $b$ all the biases, $n$ is the total number of training inputs, $a$ is the vector of outputs from the network when $x$ is the input, and the sum is over all training inputs, $x$. Of course, the output $a$ depends on $x$, $w$ and $b$, but for the sake of simplicity of the notaion I haven't indicated this. We will look at the formula of activation as we proceed with the following code.\n",
    "\n",
    "$C$ is called as the quadractic cost function or it is more popular as the Mean Squared Error (MSE) function. Inspecting the form of the quadratic cost function, we see  that $C(w, b)$ is non-negative, since every term in the sum is non-negative. Furthermore, the cost $C(w, b)$ becomes small, i.e., $C(w, b) \\approx 0$ precisely when $y(x)$ is equal to the output, $a$, for all training inputs, $x$. So we will say that our training algorithm has done a good job if it can find weights and biases so that $C(w, b) \\approx 0$. By contrast, it is not doing that well if $C(w, b)$ is large. Thus, we want to find a set of weights and biases which make the cost as small as possible. Now, we're going to develop a technique called <I><b>gradient descent</b></I> which can be used to solve such minimization problems.\n",
    "\n",
    "So, let's suppose we are trying to minimize some function, $C(v)$. This could be any real-valued function of many variables, $v = v1, v2, \\dots$. Note that I have replaced the $w$ and $b$ notation with $v$ to emphasize that it could be any function. To minimize $C(v)$ it helps to imagine $C$ as a function of two variables, $v1$ and $v2$. Now, what we'd like to find is where $C$ achieves its global minimum. We start by thinking of our function as a valley. In case of MSE, the function represents a parabolic curve and we can visualise it as a valley. And we imagine a ball rolling down the slope of the valley. Our everyday experience tells us that the ball will eventually roll to the bottom of the valley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing the parabolic shape of the MSE function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "\n",
    "plt.plot(x, x**2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping this in view, we are going to devise an algorithm to minimize C. The ball's-eye view is meant to simulate our imagination. Now, let's think about what happens when we move the ball by a small amount $\\Delta v_1$ in the $v_1$ direction, and a small amount $\\Delta v_2$ in the $v_2$ direction. Calculus tells us that $C$ changes as follows:\n",
    "$$\n",
    "        \\Delta C \\approx \\frac {\\partial C} {\\partial v_1} \\Delta v_1 + \\frac {\\partial C} {\\partial v_2} \\Delta v_2 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\dots  (eqn \\;1)\n",
    "$$\n",
    "\n",
    "So, we are going to find a way of choosing $\\Delta v_1$ and $\\Delta v_2$ so as to make $\\Delta C$ negative, i.e., we'll choose them so the ball is rolling down into the valley. To figure out how to make such a choice it helps to define $\\Delta v$ to be the vector of changes in $v$, $\\Delta v \\equiv (\\Delta v1, \\Delta v2)^T$, where T is again the transpose operation turning row vectors into column vectors. We'll also define the gradient of $C$ to be the vector of partial derivatives:\n",
    "$$\n",
    "        \\nabla C = \\left( \\frac {\\partial C} {\\partial v_1}, \\frac {\\partial C} {\\partial v_2} \\right)^T \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\dots(eqn\\;2)\n",
    "$$\n",
    "\n",
    "With these definitions in mind, the $eqn\\; 1$ can be written as:\n",
    "$$\n",
    "        \\Delta C \\approx \\nabla C \\cdot \\Delta v \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\dots (eqn\\; 3)\n",
    "$$\n",
    "\n",
    "This equation helps us explain why $\\nabla C$ is called the gradient vector: $\\nabla C$ relates changes in $v$ to changes in $C$, just as we'd expect something called a gradient to do. But what's really exciting about the equation is that it lets us see how to choose $\\Delta v$ so as to make $\\Delta C$ negative. In particular, suppose we choose\n",
    "$$\n",
    "        \\Delta v = -\\eta \\nabla C,   \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\dots                      (eqn\\; 4)\n",
    "$$\n",
    "where $\\eta$ is a small positive parameter (known as the <I><b>learning rate</b></I>). Now, $eqn\\; 2$ tells us that $\\Delta C = -\\eta \\nabla C \\cdot \\nabla C = -\\eta\\|\\nabla c\\|^2$.\n",
    "Because $\\|\\nabla c\\|^2 \\geq 0$, this guarantees that $\\Delta C < 0$, i.e., $C$ will always decrease and never increase, if we change $v$ according to prescription in $eqn\\; 4$. Now, we'll exploit this property to define the algorithm for gradient descent. That is, we'll use $eqn\\; 4$ to compute the value for $\\Delta v$, then move the ball's position $v$ by that amount:\n",
    "$$\n",
    "        v \\to v' = v - \\eta \\Delta v \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\dots                      (eqn\\; 5)\n",
    "$$\n",
    "\n",
    "Then we'll use this update rule again, to make another move. If we keep doing this, over and over, we'll keep decreasing $C$ until- we hope - we reach a global minimum.\n",
    "\n",
    "Summing up, the way the gradient descent algorithm works is to repeatedly compute the gradient $\\nabla c$, and then move in the <I>opposite</I> direction, \"falling down\" the slope of the valley.\n",
    "\n",
    "To make gradient descent work correctly, we need to choose the learnig rate $\\eta$ to be small enough that $eqn \\;3$ is a good approximation. If we don't, we might end up with $\\Delta C > 0$, which obviously would not be good for us. To put it simply, if we take a large step in the direction of the gradient then there is a risk of over shooting the minima of $C$. At the same time, we don't want $\\eta$ to be too small, since that will make the changes $\\Delta v$ tiny, and thus gradient descent algorithm will work very slowly. In practical applications, $\\eta$ is often varied so that $eqn\\; 4$ remains a good approximation, but the algorithm isn't too slow.\n",
    "\n",
    "An important thing to note here is that, I've explained gradient descent when $C$ is a function of just 2 variables. But, in fact, everything just works as well even when $C$ is a function of many more variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we apply gradient descent to learn in a neuron? The idea is to use the weights $w_k$ and biases $b_l$ which minimise the MSE cost function mentioned above. To see how this works, let's restate the gradient descent update rule, with the weights and biases replacing the variables $v_j$. In other words, our \"position\" now has components $w_k$ and $b_l$, and gradient vector $\\nabla C$ has corresponding components $\\partial C/\\partial w_k$ and $\\partial C/\\partial b_l$. Writing out the gradient descent update rule in terms of components, we have:\n",
    "$$\\begin{eqnarray}\n",
    "        w_k &\\to& w_k' &=& w_k - \\eta \\frac {\\partial C} {\\partial w_k}\\\\\n",
    "        b_l &\\to& b_l' &=& b_l - \\eta \\frac {\\partial C} {\\partial b_l}\n",
    "  \\end{eqnarray}\n",
    "$$\n",
    "        \n",
    "By repeatedly applying this update rule we can \"roll down the hill\" and hopefully find the minimum of the cost function. In other words, this is the rule which can be used to learn in a perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent:\n",
    "\n",
    "I will keep this section brief so that we can move onto the fun part which is actually coding the learning algorithms and see the implementation of aforementioned ideas in code.\n",
    "\n",
    "Notice that the cost function has the form $C = \\frac{1}{n}\\sum_x \\nabla C_x$, that is, it's an average over costs $C_x \\equiv \\frac {\\|y(x) - a\\|^2}{2}$ for individual training examples. In practice, to compute the gradient $\\nabla C$ we need to compute the gradients $\\nabla C_x$ separately for each training input, $x$, and then average them, $\\nabla C = \\frac{1}{n}\\sum_x \\nabla C_x$. Unfortunately, when the number of training inputs is very large this can take a long time, and learning thus occurs slowly.\n",
    "\n",
    "An idea called <I><b>stochastic gradient descent</b></I> can be used to speed up learning. The idea is to estimate the gradient $\\nabla C$ by computing $\\nabla C_x$ for a small sample of randomly chosen training inputs. By averaging over this small sample it turns out that we can quickly get a good estimate of the true gradient $\\nabla C$, and this helps speed up gradient descent.\n",
    "\n",
    "To make these ideas more precise, stochastic gradient descent works by randomly picking out a small number $m$ of randomly chosen training inputs. We'll label those random training inputs $X_1, X_2, \\dots, X_m$, and refer to them as a mini-batch. Provided that the sample size $m$ is large enough we expect that the average value of $\\nabla C_{X_j}$ will be roughly equal to the average over all $\\nabla C_X$, that is,\n",
    "$$\n",
    "        \\frac{\\sum_{j=1}^{m} \\nabla C_{X_j}}{m} \\approx \\frac {\\sum_x \\nabla C_x}{n} = \\nabla C,\n",
    "$$\n",
    "        \n",
    "To connect this explicitly to learning in neurons, suppose $w_k$ and $b_l$ denote the weights and biases in our neuron. Then stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs, and training with those,\n",
    "$$\\begin{eqnarray}\n",
    "        w_k &\\to& w_k' &=& w_k - \\frac {\\eta}{m} \\sum_j \\frac {\\partial C_{X_j}}{\\partial w_k}\\\\\n",
    "        b_l &\\to& b_l' &=& b_l - \\frac {\\eta}{m} \\sum_j \\frac {\\partial C_{X_j}}{\\partial b_l}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough of the theory part. The need of mentioning this theory was that as we start with Linear regression and move on to higher level algorithms we'll see that these same concepts are used to find the optimum weights and biases accross all the algorithms. What changes is the cost function $C(x)$ and the architecture of the algorithm.\n",
    "\n",
    "Let's start with the most basic algorithm of Linear Regression. We'll be implementing these algorithms both from scratch and using the MXNet utility library on the same dataset. The MXNet library provides us with a functionality called `autograd()`, which is used to calculate automatic gradients for the parameters. This functionality is very useful while calculating gradients with respect to a large number of weights and biases as is the case with Deep Neural Networks. We'll grow familiar with `autograd()` as we move through the algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
